# ДЗ 6-7
**Отчет** - https://docs.google.com/document/d/14DplkPSV4zwSlFJNJjLx0N0-6t-SFR9zadXqA106VqM/edit?usp=sharing

- hw6-7_part1.ipynb - **ЭТО ДЗ-6**:
  - здесь я перевожу данные из pdf в векторную бд, добавляю агентов + создаю датасет
  - агент1 будет генерировать вопросы на основе случайных парных чанков (допустим 333+334, 776+777)
  - агент2 будет генерировать ответы на полученные вопросы от агента 1
  - все это засунуто в датасет (лучше смотреть в приложенном csv, но ссылка вотъ)- https://drive.google.com/drive/folders/16iyweXuGoV4q7mYRQLZtujNbgujTtUz7?usp=sharing
  - была протестирована базовая модель, насколько хорошо она предсказывает модели по выбранной теме оценка STS (косинусное расстояние(Semantic Textual Similarity)) - 0.70045 (метрика от 0 до 1)
  - (но выжнее какие ответы дает модель, дообученная моделька дает ответы краткие с концентрацией информации)
- hw6-7_part2.ipynb - **ЭТО ДЗ-7**:
  - просто дообучаю модель с помощью unsloth
  - делим на train/test и тд..
  - получаем оценку STS 0.75107
  - получаем короткие + нормальные ответы
    
P.S. все написал в комменты + названия функций обозначают функционал


# ДЗ 4

- hw4_part1.ipynb - тут предобработка данных, ввод новых фичей, EDA
- hw4_part2.ipynb - тут доделываю предобработку, обучаю все модели, делаю feature importance

<br>Пункт 2 - сравнение леса и бустинга находится перед LDA
<br>Лучшая accuracy у LGBM - 60.335%

# ДЗ 3
Смотреть - hw3.ipynb

<br> Так как графики отказались показываться на гите, а чтоб сохранить модель надо ее снова обучать(если я до этого колаб закрыл), то я сделал скриншоты.
<br> Для первой модели, где евклидово расстояние - файлы eucl_X.png и тд
<br> Для второй модели, где расстояние хэмминга - файлы ham_X.png и тд
<br> Для второй модели, где манэттенское расстояние - файл manh_X.png и тд

# ДЗ 2
Смотреть - hw2_LR.ipynb и hw2_NN.ipynb

<br> Что есть что:
- hw2_LR.ipynb - сделал анализ, попробовал векторизацию + лог регрессию + пытался подобрать параметры оптюной 
- hw2_NN.ipynb - использовать сверточную нн и lstm для предсказаний
- hw2_text_normalization.ipynb - тут я почистил датасет, сделал лемматизацию для всех слов и вынес для удобства в отдельный датасет data.xlsx

<br> Что я сделать пытался, но не получилось ввиду нежелания колаба(он просто спустя 15 минут работы перезагружается, зараза) :
- хотел использовать предобученные эмбеддинги: DeepPavlov/rubert-base-cased; sberbank-ai/ruRoberta-large; Fasttext
- пытался дообучать w2v: news_upos_cbow_600_2_2018.vec

# ДЗ 1
Смотреть - hw1_gb_v2.ipynb

<br> Что есть что:
- hw1_gb.ipynb - версия фалйа только с удалением лишних символов и стоп-слов
- hw1_gb_v2.ipynb - конечная версия файла **с использованием лемматизации**

  (После проверки первого дз я сделаю rebase, удалю эти коммиты и все распределю по папкам)
