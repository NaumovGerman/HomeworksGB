# ДЗ 4

- hw4_part1.ipynb - тут предобработка данных, ввод новых фичей, EDA
- hw4_part2.ipynb - тут доделываю предобработку, обучаю все модели, делаю feature importance

<br>Пункт 2 - сравнение леса и бустинга находится перед LDA
<br>Лучшая accuracy у LGBM - 60.335%

# ДЗ 3
Смотреть - hw3.ipynb

<br> Так как графики отказались показываться на гите, а чтоб сохранить модель надо ее снова обучать(если я до этого колаб закрыл), то я сделал скриншоты.
<br> Для первой модели, где евклидово расстояние - файлы eucl_X.png и тд
<br> Для второй модели, где расстояние хэмминга - файлы ham_X.png и тд
<br> Для второй модели, где манэттенское расстояние - файл manh_X.png и тд

# ДЗ 2
Смотреть - hw2_LR.ipynb и hw2_NN.ipynb

<br> Что есть что:
- hw2_LR.ipynb - сделал анализ, попробовал векторизацию + лог регрессию + пытался подобрать параметры оптюной 
- hw2_NN.ipynb - использовать сверточную нн и lstm для предсказаний
- hw2_text_normalization.ipynb - тут я почистил датасет, сделал лемматизацию для всех слов и вынес для удобства в отдельный датасет data.xlsx

<br> Что я сделать пытался, но не получилось ввиду нежелания колаба(он просто спустя 15 минут работы перезагружается, зараза) :
- хотел использовать предобученные эмбеддинги: DeepPavlov/rubert-base-cased; sberbank-ai/ruRoberta-large; Fasttext
- пытался дообучать w2v: news_upos_cbow_600_2_2018.vec

# ДЗ 1
Смотреть - hw1_gb_v2.ipynb

<br> Что есть что:
- hw1_gb.ipynb - версия фалйа только с удалением лишних символов и стоп-слов
- hw1_gb_v2.ipynb - конечная версия файла **с использованием лемматизации**

  (После проверки первого дз я сделаю rebase, удалю эти коммиты и все распределю по папкам)
